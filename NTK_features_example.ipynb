{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUDTPingLuo/NTK/blob/main/NTK_features_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cleverhans neural_tangents"
      ],
      "metadata": {
        "id": "RJIyDWDZaRn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9352e64e-286e-486c-cda5-c4276960e8ce"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cleverhans in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: neural_tangents in /usr/local/lib/python3.11/dist-packages (0.6.5)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: pycodestyle in /usr/local/lib/python3.11/dist-packages (from cleverhans) (2.12.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from cleverhans) (3.10.0)\n",
            "Requirement already satisfied: mnist in /usr/local/lib/python3.11/dist-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cleverhans) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (from cleverhans) (0.25.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.4.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from cleverhans) (1.17.0)\n",
            "Requirement already satisfied: jax>=0.4.16 in /usr/local/lib/python3.11/dist-packages (from neural_tangents) (0.5.2)\n",
            "Requirement already satisfied: frozendict>=2.3.8 in /usr/local/lib/python3.11/dist-packages (from neural_tangents) (2.4.6)\n",
            "Requirement already satisfied: tensorflow>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from neural_tangents) (2.18.0)\n",
            "Requirement already satisfied: tf2jax>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from neural_tangents) (0.3.7)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->neural_tangents) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->neural_tangents) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.16->neural_tangents) (3.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (3.13.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.15.0->neural_tangents) (0.37.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from tf2jax>=0.3.5->neural_tangents) (0.1.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cleverhans) (2.8.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability->cleverhans) (3.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.15.0->neural_tangents) (0.45.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree>=0.1.5->tf2jax>=0.3.5->neural_tangents) (25.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->neural_tangents) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->neural_tangents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->neural_tangents) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.15.0->neural_tangents) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.15.0->neural_tangents) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.15.0->neural_tangents) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.15.0->neural_tangents) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.15.0->neural_tangents) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.15.0->neural_tangents) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "vVzNI23kaBHW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax\n",
        "import tensorflow_datasets as tfds\n",
        "from jax import scipy as sp\n",
        "from jax import vmap, grad, jacfwd\n",
        "from jax.example_libraries.stax import logsoftmax\n",
        "from cleverhans.jax.utils import clip_eta, one_hot\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_approximate_vectors(vectors_m, k):\n",
        "    \"\"\"\n",
        "    对 m 个向量组成的矩阵进行奇异值分解，选择前 k 个奇异值和对应的奇异向量，重建近似的 m x d 矩阵。\n",
        "\n",
        "    参数:\n",
        "        vectors_m (np.ndarray): m 个向量组成的矩阵，形状为 (m, d)。\n",
        "        k (int): 选择的前 k 个奇异值和奇异向量。\n",
        "\n",
        "    返回:\n",
        "        vectors_m_k (np.ndarray): 近似的 m x d 矩阵。\n",
        "        singular_values (np.ndarray): 前 k 个奇异值，形状为 (k,)。\n",
        "        singular_vectors (np.ndarray): 对应的右奇异向量，形状为 (d, k)。\n",
        "    \"\"\"\n",
        "    # 1. 对 vectors_m 进行奇异值分解\n",
        "    U, S, Vt = np.linalg.svd(vectors_m, full_matrices=False)\n",
        "\n",
        "    # 2. 选择前 k 个奇异值和对应的奇异向量\n",
        "    U_k = U[:, :k]  # 左奇异向量，形状为 (m, k)\n",
        "    S_k = S[:k]  # 奇异值，形状为 (k,)\n",
        "    Vt_k = Vt[:k, :]  # 右奇异向量，形状为 (k, d)\n",
        "\n",
        "    # 3. 重建近似的 m x d 矩阵\n",
        "    vectors_m_k = np.dot(U_k, np.dot(np.diag(S_k), Vt_k))\n",
        "\n",
        "    return vectors_m_k, S_k, Vt_k.T  # Vt_k.T 是右奇异向量的转置，形状为 (d, k)\n",
        "\n",
        "\n",
        "def compute_full_matrix(vectors_n, vectors_m_k):\n",
        "    \"\"\"\n",
        "    将 n 个向量和近似的 m 个向量组合成 (m+n) x d 矩阵，并计算 (m+n) x (m+n) 的内积矩阵。\n",
        "\n",
        "    参数:\n",
        "        vectors_n (np.ndarray): n 个向量组成的矩阵，形状为 (n, d)。\n",
        "        vectors_m_k (np.ndarray): 近似的 m 个向量组成的矩阵，形状为 (m, d)。\n",
        "\n",
        "    返回:\n",
        "        full_matrix (np.ndarray): (m+n) x (m+n) 的内积矩阵。\n",
        "    \"\"\"\n",
        "    # 1. 组合成 (m+n) x d 矩阵\n",
        "    combined_vectors = np.vstack((vectors_n, vectors_m_k))  # 形状为 (m+n, d)\n",
        "\n",
        "    # 2. 计算 (m+n) x (m+n) 的内积矩阵\n",
        "    full_matrix = np.dot(combined_vectors, combined_vectors.T)\n",
        "\n",
        "    return full_matrix\n",
        "\n",
        "def compute_eigenvalues(matrix):\n",
        "    \"\"\"\n",
        "    计算矩阵的特征值。\n",
        "    \"\"\"\n",
        "    return np.linalg.eigvals(matrix)\n",
        "\n",
        "# 示例用法\n",
        "if __name__ == \"__main__\":\n",
        "    # 随机生成数据\n",
        "    n = 5  # n 个向量\n",
        "    m = 4  # m 个向量\n",
        "    d = 3  # 向量维度\n",
        "    vectors_n = np.random.rand(n, d)  # 随机生成 n 个 d 维向量\n",
        "    vectors_m = np.random.rand(m, d)  # 随机生成 m 个 d 维向量\n",
        "\n",
        "    # 选择前 k 个奇异值\n",
        "    k = 2\n",
        "\n",
        "    # 对 m 个向量进行奇异值分解，并重建近似的 m x d 矩阵\n",
        "    vectors_m_k, singular_values, singular_vectors = compute_approximate_vectors(vectors_m, k)\n",
        "\n",
        "    # 计算 (m+n) x (m+n) 的内积矩阵\n",
        "    full_matrix_ori = compute_full_matrix(vectors_n, vectors_m)\n",
        "    full_matrix = compute_full_matrix(vectors_n, vectors_m_k)\n",
        "\n",
        "    # 计算特征值\n",
        "    eigenvalues_ori = compute_eigenvalues(full_matrix_ori)  # 原来的矩阵特征值\n",
        "    eigenvalues = compute_eigenvalues(full_matrix)    # 近似后的矩阵特征值\n",
        "\n",
        "    # 打印结果\n",
        "    print(\"m 个向量的原始矩阵:\")\n",
        "    print(vectors_m)\n",
        "\n",
        "    print(\"\\n重建的近似的 m x d 矩阵:\")\n",
        "    print(vectors_m_k)\n",
        "\n",
        "    print(\"\\n(m+n) x (m+n) 的原始内积矩阵:\")\n",
        "    print(full_matrix_ori)\n",
        "\n",
        "    print(\"\\n(m+n) x (m+n) 的内积矩阵:\")\n",
        "    print(full_matrix)\n",
        "\n",
        "    print(\"\\n原来的 (m+n)*(m+n) 矩阵的特征值：\")\n",
        "    print(eigenvalues_ori)\n",
        "\n",
        "    print(\"\\n近似后的 (m+n)*(m+n) 矩阵的特征值：\")\n",
        "    print(eigenvalues)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNGW0ySDxPFG",
        "outputId": "4bd2b670-9700-4646-aa60-58acb626b346"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m 个向量的原始矩阵:\n",
            "[[0.30452243 0.57166672 0.29151231]\n",
            " [0.94269921 0.29384953 0.60849727]\n",
            " [0.60245439 0.18692661 0.8326985 ]\n",
            " [0.12736767 0.22922813 0.91276993]]\n",
            "\n",
            "重建的近似的 m x d 矩阵:\n",
            "[[0.42914831 0.19864914 0.35035697]\n",
            " [0.908275   0.39688457 0.59224318]\n",
            " [0.55961821 0.31513952 0.81247252]\n",
            " [0.13991501 0.1916727  0.91869441]]\n",
            "\n",
            "(m+n) x (m+n) 的原始内积矩阵:\n",
            "[[0.70349524 0.39465655 0.34483592 0.55397415 0.53916693 0.5147106\n",
            "  0.63730558 0.69680357 0.72685475]\n",
            " [0.39465655 0.3806802  0.34949228 0.31701011 0.40191068 0.39265839\n",
            "  0.28914855 0.23739321 0.25293703]\n",
            " [0.34483592 0.34949228 0.64199463 0.51081223 0.86487497 0.49284415\n",
            "  0.73164003 0.47583349 0.21640455]\n",
            " [0.55397415 0.31701011 0.51081223 0.60631238 0.79497688 0.51044716\n",
            "  0.85335328 0.75004926 0.57490025]\n",
            " [0.53916693 0.40191068 0.86487497 0.79497688 1.2662549  0.67745751\n",
            "  1.21007739 0.88567478 0.47895039]\n",
            " [0.5147106  0.39265839 0.49284415 0.51044716 0.67745751 0.50451618\n",
            "  0.63244149 0.53306246 0.43591208]\n",
            " [0.63730558 0.28914855 0.73164003 0.85335328 1.21007739 0.63244149\n",
            "  1.34529827 1.12955633 0.74284599]\n",
            " [0.69680357 0.23739321 0.47583349 0.75004926 0.88567478 0.53306246\n",
            "  1.12955633 1.09127964 0.8796442 ]\n",
            " [0.72685475 0.25293703 0.21640455 0.57490025 0.47895039 0.43591208\n",
            "  0.74284599 0.8796442  0.901917  ]]\n",
            "\n",
            "(m+n) x (m+n) 的内积矩阵:\n",
            "[[0.70349524 0.39465655 0.34483592 0.55397415 0.53916693 0.3716316\n",
            "  0.67682691 0.74598241 0.71244955]\n",
            " [0.39465655 0.3806802  0.34949228 0.31701011 0.40191068 0.17958842\n",
            "  0.34800281 0.31062922 0.23148514]\n",
            " [0.34483592 0.34949228 0.64199463 0.51081223 0.86487497 0.3691614\n",
            "  0.76580372 0.5183455  0.20395216]\n",
            " [0.55397415 0.31701011 0.51081223 0.60631238 0.79497688 0.44834311\n",
            "  0.87050768 0.77139555 0.56864761]\n",
            " [0.53916693 0.40191068 0.86487497 0.79497688 1.2662549  0.60522649\n",
            "  1.23002906 0.91050189 0.47167816]\n",
            " [0.3716316  0.17958842 0.3691614  0.44834311 0.60522649 0.34637976\n",
            "  0.67612199 0.58741681 0.4199909 ]\n",
            " [0.67682691 0.34800281 0.76580372 0.87050768 1.23002906 0.67612199\n",
            "  1.33323283 1.11454256 0.74724375]\n",
            " [0.74598241 0.31062922 0.5183455  0.77139555 0.91050189 0.58741681\n",
            "  1.11454256 1.07259706 0.8851166 ]\n",
            " [0.71244955 0.23148514 0.20395216 0.56864761 0.47167816 0.4199909\n",
            "  0.74724375 0.8851166  0.90031405]]\n",
            "\n",
            "原来的 (m+n)*(m+n) 矩阵的特征值：\n",
            "[ 5.85962259e+00+0.00000000e+00j  9.88258887e-01+0.00000000e+00j\n",
            "  5.93866958e-01+0.00000000e+00j -3.23353595e-16+0.00000000e+00j\n",
            "  9.35765427e-17+5.58226702e-17j  9.35765427e-17-5.58226702e-17j\n",
            " -5.36034967e-17+7.43833228e-17j -5.36034967e-17-7.43833228e-17j\n",
            "  3.76426537e-18+0.00000000e+00j]\n",
            "\n",
            "近似后的 (m+n)*(m+n) 矩阵的特征值：\n",
            "[ 5.85418760e+00+0.00000000e+00j  9.84367550e-01+0.00000000e+00j\n",
            "  4.12705899e-01+0.00000000e+00j  3.10422726e-16+0.00000000e+00j\n",
            " -2.50649046e-16+0.00000000e+00j -4.94401954e-17+2.45662044e-17j\n",
            " -4.94401954e-17-2.45662044e-17j  1.21162780e-16+0.00000000e+00j\n",
            "  6.09394221e-17+0.00000000e+00j]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWgB_hZpaBHY"
      },
      "source": [
        "### Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "A83_vcWL0__f"
      },
      "outputs": [],
      "source": [
        "DATASET = 'cifar10' #@param ['cifar10', 'mnist']\n",
        "TASK = 'binary' #@param ['binary', 'multiclass']\n",
        "TRAIN_BATCH_SIZE = 2000 #@param\n",
        "TEST_BATCH_SIZE = 100 #@param\n",
        "RANDOM_LABELS = False\n",
        "NORMLZ = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3cTvj4aBHa"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SM1yc9liaBHa"
      },
      "outputs": [],
      "source": [
        "def normalize_img(tensor):\n",
        "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
        "    return tensor\n",
        "\n",
        "def binarize_labels(labels):\n",
        "    labels = np.array(labels)\n",
        "    bin = lambda x: -1 if x == 0 else 1\n",
        "    return jnp.array(np.vectorize(bin)(labels))\n",
        "\n",
        "def select_data(images, _labels, class1, class2):\n",
        "    idx = np.logical_or(_labels == class1, _labels == class2)\n",
        "    images = images[idx]\n",
        "    labels = _labels[idx]\n",
        "    return images, labels\n",
        "\n",
        "def get_tfds_dataset(name):\n",
        "    assert name in ['cifar10', 'mnist']\n",
        "    ds_train, ds_test = tfds.as_numpy(\n",
        "      tfds.load(\n",
        "          name,\n",
        "          split=['train', 'test'],\n",
        "          batch_size=-1,\n",
        "          as_dataset_kwargs={'shuffle_files': True}))\n",
        "\n",
        "    return ds_train['image'], ds_train['label'], ds_test['image'], ds_test['label']\n",
        "\n",
        "def one_hot(x,\n",
        "            num_classes,\n",
        "            center=False,\n",
        "            dtype=np.float32):\n",
        "    assert len(x.shape) == 1\n",
        "    one_hot_vectors = np.array(x[:, None] == np.arange(num_classes), dtype)\n",
        "    if center:\n",
        "        one_hot_vectors = one_hot_vectors - 1. / num_classes\n",
        "    return one_hot_vectors\n",
        "\n",
        "def _fast_gradient_method(\n",
        "    model_fn, x, eps, norm, clip_min=None, clip_max=None, y=None, binary=True\n",
        "):\n",
        "\n",
        "    if y is None:\n",
        "        # Using model predictions as ground truth to avoid label leaking\n",
        "        if (binary):\n",
        "            y = jnp.sign(model_fn(x))\n",
        "        else:\n",
        "            x_labels = jnp.argmax(model_fn(x), axis=1)\n",
        "            y = one_hot(x_labels, 10)\n",
        "\n",
        "\n",
        "    def loss_adv_multi(image, label):\n",
        "        pred = model_fn(jnp.expand_dims(image, axis=0))\n",
        "        loss = -jnp.sum(logsoftmax(pred) * label)\n",
        "        return loss\n",
        "\n",
        "    def bce_w_logits(img, y, weight=None, average=False):\n",
        "        \"\"\"\n",
        "        Binary Cross Entropy Loss\n",
        "        Should be numerically stable, built based on: https://github.com/pytorch/pytorch/issues/751\n",
        "        :param x: Input tensor\n",
        "        :param y: Target tensor\n",
        "        :param weight: Vector of example weights\n",
        "        :param average: Boolean to average resulting loss vector\n",
        "        :return: Scalar value\n",
        "        \"\"\"\n",
        "        x = model_fn(img)\n",
        "        if (-1. in y):\n",
        "            y = (y+1)/2\n",
        "        max_val = jnp.clip(x, 0, None)\n",
        "        loss = x - x * y + max_val + jnp.log(jnp.exp(-max_val) + jnp.exp((-x - max_val)))\n",
        "\n",
        "        if weight is not None:\n",
        "            loss = loss * weight\n",
        "\n",
        "        if average:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss.sum()\n",
        "\n",
        "    avoid_zero = 1e-3\n",
        "    if (binary):\n",
        "        grads_fn = grad(bce_w_logits)\n",
        "    else:\n",
        "        grads_fn = vmap(grad(loss_adv_multi), in_axes=(0, 0), out_axes=0)\n",
        "    # works for find_rob_kernel\n",
        "    grads = grads_fn(x, y)\n",
        "\n",
        "    axis = list(range(1, len(grads.shape)))\n",
        "    avoid_zero_div = 1e-12\n",
        "    if norm == np.inf:\n",
        "        perturbation = eps * jnp.sign(grads)\n",
        "    elif norm == 2:\n",
        "        square = np.maximum(\n",
        "            avoid_zero_div, np.sum(np.square(grads), axis=axis, keepdims=True)\n",
        "        )\n",
        "        perturbation = grads / np.sqrt(square)\n",
        "\n",
        "    adv_x = x + perturbation\n",
        "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
        "    if (clip_min is not None) or (clip_max is not None):\n",
        "        # We don't currently support one-sided clipping\n",
        "        assert clip_min is not None and clip_max is not None\n",
        "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
        "    return adv_x, grads\n",
        "\n",
        "def _projected_gradient_descent(\n",
        "    model_fn,\n",
        "    x,\n",
        "    eps,\n",
        "    eps_iter,\n",
        "    nb_iter,\n",
        "    norm,\n",
        "    clip_min=None,\n",
        "    clip_max=None,\n",
        "    y=None,\n",
        "    binary=True,\n",
        "    rand_init=None,\n",
        "    rand_minmax=0.3,\n",
        "):\n",
        "    \"\"\"\n",
        "    This class implements either the Basic Iterative Method\n",
        "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
        "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
        "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
        "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
        "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
        "    :param x: input tensor.\n",
        "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
        "    :param eps_iter: step size for each attack iteration\n",
        "    :param nb_iter: Number of attack iterations.\n",
        "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
        "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
        "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
        "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
        "              target label. Otherwise, only provide this parameter if you'd like to use true\n",
        "              labels when crafting adversarial samples. Otherwise, model predictions are used\n",
        "              as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
        "              https://arxiv.org/abs/1611.01236). Default is None.\n",
        "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
        "              Untargeted, the default, will try to make the label incorrect.\n",
        "              Targeted will instead try to move in the direction of being more like y.\n",
        "    :return: a tensor for the adversarial example\n",
        "    \"\"\"\n",
        "\n",
        "    assert eps_iter <= eps, (eps_iter, eps)\n",
        "    if norm == 1:\n",
        "        raise NotImplementedError(\n",
        "            \"It's not clear that FGM is a good inner loop\"\n",
        "            \" step for PGD when norm=1, because norm=1 FGM \"\n",
        "            \" changes only one pixel at a time. We need \"\n",
        "            \" to rigorously test a strong norm=1 PGD \"\n",
        "            \"before enabling this feature.\"\n",
        "        )\n",
        "    if norm not in [np.inf, 2]:\n",
        "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
        "\n",
        "    # Initialize loop variables\n",
        "    if rand_init:\n",
        "        rand_minmax = eps\n",
        "        eta = np.random.uniform(x.shape, -rand_minmax, rand_minmax)\n",
        "    else:\n",
        "        eta = np.zeros_like(x)\n",
        "\n",
        "    # Clip eta\n",
        "    eta = clip_eta(eta, norm, eps)\n",
        "    adv_x = x + eta\n",
        "    if clip_min is not None or clip_max is not None:\n",
        "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
        "\n",
        "    if y is None:\n",
        "        # Using model predictions as ground truth to avoid label leaking\n",
        "        x_labels = np.argmax(model_fn(x), 1)\n",
        "        y = one_hot(x_labels, 10)\n",
        "\n",
        "    for _ in range(nb_iter):\n",
        "        adv_x, _ = _fast_gradient_method(\n",
        "            model_fn,\n",
        "            adv_x,\n",
        "            eps_iter,\n",
        "            norm,\n",
        "            clip_min=clip_min,\n",
        "            clip_max=clip_max,\n",
        "            y=y,\n",
        "            binary=binary\n",
        "        )\n",
        "\n",
        "        # Clipping perturbation eta to norm norm ball\n",
        "        eta = adv_x - x\n",
        "        eta = clip_eta(eta, norm, eps)\n",
        "        adv_x = x + eta\n",
        "\n",
        "        # Redo the clipping.\n",
        "        # FGM already did it, but subtracting and re-adding eta can add some\n",
        "        # small numerical error.\n",
        "        if clip_min is not None or clip_max is not None:\n",
        "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
        "\n",
        "    return adv_x\n",
        "\n",
        "class OverCompleteKernel():\n",
        "\n",
        "    def __init__(self, kernel_fn, x_train, y_train):\n",
        "        _kernel_fn = nt.batch(kernel_fn, device_count=-1, batch_size=50)\n",
        "        self.KERNEL_FN = lambda x, x_: kernel_fn(x, x_, 'ntk')\n",
        "        self._KERNEL_FN = lambda x, x_: _kernel_fn(x, x_, 'ntk')\n",
        "        self.k_ss = self.KERNEL_FN(x_train, x_train)\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.inv_k_ss = None\n",
        "        self.l = None\n",
        "        self.v = None\n",
        "\n",
        "\n",
        "    def init_from_matrix(self, kernel_fn, kernel, x_train, y_train):\n",
        "        self.KERNEL_FN = kernel_fn\n",
        "        self.k_ss = kernel\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.inv_k_ss = None\n",
        "        self.l = None\n",
        "        self.v = None\n",
        "\n",
        "    def compute_eig_kernel(self, eig_idx): # temporary fix for OOM bug\n",
        "        if (self.l == None):\n",
        "            self.l, self.v = jnp.linalg.eigh(self.inv_k_ss)\n",
        "        V = jnp.array(jnp.take(self.v, eig_idx, axis=1), ndmin=2)\n",
        "        Lambda = jnp.array(jnp.diag(jnp.take(self.l, eig_idx).reshape(-1)), ndmin=2)\n",
        "        Lambda_diag = jnp.diag(Lambda)\n",
        "        sorted_diag = jnp.sort(Lambda_diag)[::-1]\n",
        "        print('对角线元素从大到小排序：', sorted_diag)\n",
        "\n",
        "        # 绘制直方图\n",
        "        plt.hist(sorted_diag, bins=30, edgecolor='black', alpha=0.7)\n",
        "        plt.xlabel(\"Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(\"Value Distribution (JAX)\")\n",
        "        plt.show()\n",
        "\n",
        "        sorted_indices = jnp.argsort(Lambda_diag)[::-1]  # 先升序再翻转\n",
        "        # 选取特征值在10到20之间的索引\n",
        "        # filtered_indices = sorted_indices[jnp.where((Lambda_diag[sorted_indices] >= 20) & (Lambda_diag[sorted_indices] <= 90))]\n",
        "        top_k = 10  # 需要前 100 个\n",
        "        bottom_k = 10  # 需要后 100 个\n",
        "\n",
        "        # 如果元素不足 100，则取最大可能的数量\n",
        "        num_elements = len(sorted_indices)\n",
        "        top_part = sorted_indices[:min(top_k, num_elements)]  # 取前100个\n",
        "        bottom_part = sorted_indices[-min(bottom_k, num_elements):]  # 取后100个\n",
        "\n",
        "        # 组合索引\n",
        "        final_indices = jnp.concatenate([top_part, bottom_part])\n",
        "\n",
        "        # # 设定 top_k 和 random_k\n",
        "        # top_k = 10  # 取前 100 个\n",
        "        # random_k = 10  # 在剩余元素中随机选 100 个\n",
        "\n",
        "        # # 1️⃣ 取前 100 个元素\n",
        "        # num_elements = len(sorted_indices)\n",
        "        # top_part = sorted_indices[:min(top_k, num_elements)]  # 取前 100 个\n",
        "\n",
        "        # # 2️⃣ 取剩余元素\n",
        "        # remaining_elements = sorted_indices[min(top_k, num_elements):]  # 剩余部分\n",
        "\n",
        "        # # 3️⃣ 在剩余元素中随机选 100 个\n",
        "        # key = random.PRNGKey(0)  # JAX 随机种子\n",
        "        # random_part = random.choice(key, remaining_elements, shape=(min(random_k, len(remaining_elements)),), replace=False)\n",
        "\n",
        "        # # 4️⃣ 合并最终索引\n",
        "        # final_indices = jnp.concatenate([top_part, random_part])\n",
        "\n",
        "        # eig_idx = final_indices\n",
        "        eig_idx = [0,1,2,3,4,5,6,7,8,9,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999]\n",
        "        V = jnp.array(jnp.take(self.v, eig_idx, axis=1), ndmin=2)\n",
        "        Lambda = jnp.array(jnp.diag(jnp.take(self.l, eig_idx).reshape(-1)), ndmin=2)\n",
        "        print('lambda:', Lambda)\n",
        "        try:\n",
        "            assert eig_idx.shape[0] > 1\n",
        "            return V @ Lambda @ V.T\n",
        "        except:\n",
        "            return V.reshape(-1, 1) @ Lambda.reshape(1, 1) @ V.T.reshape(1, -1)\n",
        "\n",
        "\n",
        "    def model_fn(self, eig_idx=None):\n",
        "        if (eig_idx is not None):\n",
        "            if (self.l is None):\n",
        "                self.inv_k_ss = jnp.linalg.inv(self.k_ss)\n",
        "            _inv_idx = self.compute_eig_kernel(eig_idx)\n",
        "            train_component = _inv_idx @ self.y_train\n",
        "        else:\n",
        "            _k_ss = self.k_ss\n",
        "            print('核矩阵:', _k_ss.shape)\n",
        "            train_component = sp.linalg.solve(_k_ss, self.y_train, assume_a='pos')\n",
        "            # train_component = train_component[:1000]\n",
        "            print(train_component)\n",
        "\n",
        "        def model(images):\n",
        "            return jnp.dot(self.KERNEL_FN(images, self.x_train), train_component)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "def FullyConnectedNetwork(\n",
        "    depth,\n",
        "    width,\n",
        "    W_std = np.sqrt(2),\n",
        "    b_std = 0.1,\n",
        "    num_classes = 10,\n",
        "    parameterization = 'ntk',\n",
        "    activation = 'relu'):\n",
        "  \"\"\"Returns neural_tangents.stax fully connected network.\"\"\"\n",
        "  activation_fn = stax.Relu()\n",
        "  dense = functools.partial(\n",
        "      stax.Dense, W_std=W_std, b_std=b_std, parameterization=parameterization)\n",
        "\n",
        "  layers = [stax.Flatten()]\n",
        "  for _ in range(depth):\n",
        "    layers += [dense(width), activation_fn]\n",
        "  layers += [stax.Dense(num_classes, W_std=W_std, b_std=b_std,\n",
        "                        parameterization=parameterization)]\n",
        "\n",
        "  return stax.serial(*layers)\n",
        "\n",
        "def FullyConvolutionalNetwork(\n",
        "    depth,\n",
        "    width,\n",
        "    W_std = np.sqrt(2),\n",
        "    b_std = 0.1,\n",
        "    num_classes = 10,\n",
        "    parameterization = 'ntk',\n",
        "    activation = 'relu'):\n",
        "  \"\"\"Returns neural_tangents.stax fully convolutional network.\"\"\"\n",
        "  activation_fn = stax.Relu()\n",
        "  layers = []\n",
        "  conv = functools.partial(\n",
        "      stax.Conv,\n",
        "      W_std=W_std,\n",
        "      b_std=b_std,\n",
        "      padding='SAME',\n",
        "      parameterization=parameterization)\n",
        "\n",
        "  for _ in range(depth):\n",
        "    layers += [conv(width, (3,3)), activation_fn]\n",
        "  layers += [stax.Flatten(), stax.Dense(num_classes, W_std=W_std, b_std=b_std,\n",
        "                                        parameterization=parameterization)]\n",
        "\n",
        "  return stax.serial(*layers)\n",
        "\n",
        "def MyrtleNetwork(\n",
        "    depth,\n",
        "    width,\n",
        "    W_std = np.sqrt(2),\n",
        "    b_std = 0.1,\n",
        "    num_classes = 10,\n",
        "    parameterization = 'ntk',\n",
        "    activation = 'relu'):\n",
        "  \"\"\"Returns neural_tangents.stax Myrtle network.\"\"\"\n",
        "  layer_factor = {5: [1, 1, 1], 7: [1, 2, 2], 10: [2, 3, 3]}\n",
        "  if depth not in layer_factor.keys():\n",
        "    raise NotImplementedError(\n",
        "        'Myrtle network withd depth %d is not implemented!' % depth)\n",
        "  activation_fn = stax.Relu()\n",
        "  layers = []\n",
        "  conv = functools.partial(\n",
        "      stax.Conv,\n",
        "      W_std=W_std,\n",
        "      b_std=b_std,\n",
        "      padding='SAME',\n",
        "      parameterization=parameterization)\n",
        "  layers += [conv(width, (3, 3)), activation_fn]\n",
        "\n",
        "  # generate blocks of convolutions followed by average pooling for each\n",
        "  # layer of layer_factor except the last\n",
        "  for block_depth in layer_factor[depth][:-1]:\n",
        "    for _ in range(block_depth):\n",
        "      layers += [conv(width, (3, 3)), activation_fn]\n",
        "    layers += [stax.AvgPool((2, 2), strides=(2, 2))]\n",
        "\n",
        "  # generate final blocks of convolution followed by global average pooling\n",
        "  for _ in range(layer_factor[depth][-1]):\n",
        "    layers += [conv(width, (3, 3)), activation_fn]\n",
        "  layers += [stax.GlobalAvgPool()]\n",
        "\n",
        "  layers += [\n",
        "      stax.Dense(num_classes, W_std, b_std, parameterization=parameterization)\n",
        "  ]\n",
        "\n",
        "  return stax.serial(*layers)\n",
        "\n",
        "def get_kernel_fn(architecture, depth, width=1000, parameterization='ntk', task='binary'):\n",
        "  if (task == 'binary'):\n",
        "      num_classes = 1\n",
        "  else:\n",
        "      num_classes = 10\n",
        "  if architecture == 'FC':\n",
        "    return FullyConnectedNetwork(depth=depth, width=width, parameterization=parameterization, num_classes=num_classes)\n",
        "  elif architecture == 'Conv':\n",
        "    return FullyConvolutionalNetwork(depth=depth, width=width, parameterization=parameterization, num_classes=num_classes)\n",
        "  elif architecture == 'Myrtle':\n",
        "    return MyrtleNetwork(depth=depth, width=width, parameterization=parameterization, num_classes=num_classes)\n",
        "  else:\n",
        "    raise NotImplementedError(f'Unrecognized architecture {architecture}')\n",
        "\n",
        "def model_eval(model_eig, i, x_test, y_test, img_index, i_to_label, suppress=False):\n",
        "    binary = True if TASK == 'binary' else False\n",
        "    print(f'----||--Eigenvector {i} stats--||----\\n')\n",
        "    print('Accuracy on cars', jnp.sum(jnp.logical_and(jnp.sign(model_eig(x_test)) == y_test, y_test == -jnp.ones_like(y_test))) / jnp.sum(y_test == -jnp.ones_like(y_test)))\n",
        "    print('Accuracy on airplanes', jnp.sum(jnp.logical_and(jnp.sign(model_eig(x_test)) == y_test, y_test == jnp.ones_like(y_test))) / jnp.sum(y_test == jnp.ones_like(y_test)))\n",
        "    # Natural accuracy\n",
        "    if (TASK == 'binary'):\n",
        "        assert model_eig(x_test).shape == (TEST_BATCH_SIZE,)\n",
        "        nat_acc = jnp.mean(jnp.sign(model_eig(x_test)) == y_test)\n",
        "    else:\n",
        "        assert model_eig(x_test).shape[0] == TEST_BATCH_SIZE and model_eig(x_test).shape[1] == 10\n",
        "        nat_acc = jnp.mean(model_eig(x_test).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "    print('Total accuracy', nat_acc)\n",
        "    # Robust accuracy\n",
        "    adv_x, grads = _fast_gradient_method(model_eig, x_test, eps=8/255, norm=np.inf, clip_min=0, clip_max=1, y=y_test, binary=binary)\n",
        "    if (TASK == 'binary'):\n",
        "        assert model_eig(x_test).shape == (TEST_BATCH_SIZE,)\n",
        "        rob_acc = jnp.mean(jnp.sign(model_eig(adv_x)) == y_test)\n",
        "    else:\n",
        "        assert model_eig(x_test).shape[0] == TEST_BATCH_SIZE and model_eig(x_test).shape[1] == 10\n",
        "        rob_acc = jnp.mean(model_eig(adv_x).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "    print(f\"Robust accuracy (using test labels) {rob_acc}\")\n",
        "\n",
        "    if (suppress):\n",
        "        return nat_acc, rob_acc\n",
        "    plt.imshow((normalize_img(grads[img_index])))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print('\\n')\n",
        "    return nat_acc, rob_acc, grads\n",
        "\n",
        "img_index = 42\n",
        "i_to_label = {0: 'airplane', 1:  'car'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5619pQaBHf"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrwCLYdZaBHf"
      },
      "outputs": [],
      "source": [
        "X_TRAIN_RAW, LABELS_TRAIN, X_TEST_RAW, LABELS_TEST = get_tfds_dataset(DATASET)\n",
        "if (TASK == 'binary'):\n",
        "    X_TRAIN_RAW, LABELS_TRAIN = select_data(X_TRAIN_RAW, LABELS_TRAIN, 0, 1) # please only use 0 and 1 for classes - otherwise change binarize_labels, too\n",
        "    X_TEST_RAW, LABELS_TEST = select_data(X_TEST_RAW, LABELS_TEST, 0, 1)\n",
        "    Y_TRAIN = binarize_labels(LABELS_TRAIN)\n",
        "    Y_TEST = binarize_labels(LABELS_TEST)\n",
        "else:\n",
        "    Y_TRAIN, Y_TEST = one_hot(LABELS_TRAIN, 10, center=False), one_hot(LABELS_TEST, 10, center=False)\n",
        "\n",
        "X_TRAIN, X_TEST = X_TRAIN_RAW / 255, X_TEST_RAW / 255\n",
        "print(len(X_TRAIN), len(X_TEST))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ikoUC7h9KFE"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = X_TRAIN[:TRAIN_BATCH_SIZE], Y_TRAIN[:TRAIN_BATCH_SIZE]\n",
        "# print(x_train)\n",
        "x_test, y_test = X_TEST[:TEST_BATCH_SIZE], Y_TEST[:TEST_BATCH_SIZE]\n",
        "print(len(x_train), len(x_test))\n",
        "# x_train_re = x_train.reshape(2000, -1)\n",
        "# x_train_ori = x_train_re.reshape((2000, 32, 32, 3))\n",
        "# print(x_train_ori)\n",
        "# print(x_train_re.shape,x_train_ori.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-JxejxHRBQo"
      },
      "source": [
        "## Kernel inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXtA1iPqyONN"
      },
      "outputs": [],
      "source": [
        "def fit(x_train, y_train, x_test, y_test, TASK, depth=2):\n",
        "    binary = True if TASK == 'binary' else False\n",
        "    aux = []\n",
        "    _, _, kernel_fn = get_kernel_fn('FC', depth=depth, task=TASK)\n",
        "    kernel_obj = OverCompleteKernel(kernel_fn, x_train, y_train)\n",
        "    eig_idx = jnp.arange(2000)\n",
        "    model = kernel_obj.model_fn(eig_idx)\n",
        "    print(\"Full kernel stats\")\n",
        "    if (TASK == 'binary'):\n",
        "#         assert model(x_test).shape == (x_test.shape[0],)\n",
        "        _acc = jnp.mean(jnp.sign(model(x_test)) == y_test)\n",
        "        aux.append(_acc)\n",
        "    else:\n",
        "#         assert model(x_test).shape[0] == x_test.shape[0] and model(x_test).shape[1] == 10\n",
        "        _acc = jnp.mean(model(x_test).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "        aux.append(_acc)\n",
        "    print(f\"Natural accuracy {_acc}\")\n",
        "    adv_x, grad = _fast_gradient_method(model, x_test, eps=8/255, norm=np.inf, clip_min=0, clip_max=1, y=y_test)\n",
        "    adv_x_pgd = _projected_gradient_descent(model, x_test, eps=8/255, eps_iter=0.01, nb_iter=10, norm=np.inf, clip_min=0, clip_max=1, y=y_test)\n",
        "    if (TASK == 'binary'):\n",
        "#         assert model(adv_x).shape == (x_test.shape[0],)\n",
        "        _acc = jnp.mean(jnp.sign(model(adv_x)) == y_test)\n",
        "        aux.append(_acc)\n",
        "        print(f'Robust fgsm accuracy (using test labels) {_acc}')\n",
        "        _acc = jnp.mean(jnp.sign(model(adv_x_pgd)) == y_test)\n",
        "        aux.append(_acc)\n",
        "    else:\n",
        "#         assert model(adv_x).shape[0] == x_test.shape[0] and model(adv_x).shape[1] == 10\n",
        "        _acc = jnp.mean(model(adv_x).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "        aux.append(_acc)\n",
        "        print(f'Robust fgsm accuracy (using test labels) {_acc}')\n",
        "        _acc = jnp.mean(model(adv_x_pgd).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "        aux.append(_acc)\n",
        "    print(f\"Robust pgd accuracy (using test labels) {_acc}\")\n",
        "    return kernel_obj, model, aux, adv_x, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VmRWM_BaBHh"
      },
      "outputs": [],
      "source": [
        "kernel_obj, model, aux, adv_x, grads = fit(x_train, y_train, x_test, y_test, TASK, depth=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thDsO6RuaBHh"
      },
      "source": [
        "### Show different NTK features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWq8CSyjaBHh"
      },
      "outputs": [],
      "source": [
        "new_grads = {}\n",
        "ntk_feats_indx = [0, 1, 2, 3, 4, 5, 10, 25, 1500, 1987, 1988, 1998] # change these if you'd like\n",
        "for j in ntk_feats_indx:\n",
        "    model_check = kernel_obj.model_fn(jnp.array(j))\n",
        "    _, _, new_grads[j] = model_eval(model_check, str(j), x_test, y_test, img_index, i_to_label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgwJirtzbNA1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}